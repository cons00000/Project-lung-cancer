{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d652d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xai _ Grad-Cam (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbb18f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/users/pred_lung_cancer/piquet_con/Project-lung-cancer/src/Segmentation')\n",
    "from TheDuneAI import ContourPilot as cp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# Import for viewer\n",
    "import nrrd\n",
    "import napari\n",
    "\n",
    "# Imports for Xplique\n",
    "import requests\n",
    "from PIL import Image\n",
    "BATCH_SIZE = 8\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor, resize\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import xplique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8cf39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize the data for one patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1150f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/Users/constance/Documents/Project_lung_cancer/src/Segmentation/model_files'\n",
    "path_to_test_data = '/Users/constance/Documents/Project_lung_cancer/NIH dataset_raw/NRRD/converted_nrrds/'\n",
    "save_path = '/Users/constance/Documents/Project_lung_cancer/NIH dataset_raw/Processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4351f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = cp(model_path,path_to_test_data,save_path,verbosity=True)\n",
    "for i, layer in enumerate(model.model1.layers):\n",
    "        print(i, layer.name, layer.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaed4f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_with_progress = tqdm(model.Patients_gen, desc='Progress')\n",
    "gen_iterator = iter(gen_with_progress)  # <-- transformer en itÃ©rateur\n",
    "img, _, filename, params = next(gen_iterator)\n",
    "print(img.shape) \n",
    "\n",
    "# Select the middle slice of the 3D image\n",
    "print(f\"Image shape: {img.shape}\") \n",
    "img=img.squeeze()  \n",
    "print(f\"Image shape after squeeze: {img.shape}\") \n",
    "# --- Select the middle slice ---\n",
    "middle_index = img.shape[0] // 2\n",
    "print(f\"Middle slice index: {middle_index}\")\n",
    "middle_slice = img[middle_index, :, :]  # shape: (512, 512)\n",
    "\n",
    "mask = model.model1.predict(img, batch_size=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6b35d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lire les fichiers NRRD\n",
    "data_image, header_image = nrrd.read(img)\n",
    "data_mask, header_mask = nrrd.read(mask)\n",
    "\n",
    "# Lancer napari et afficher les images\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Ajouter l'image CT (en utilisant des niveaux de gris)\n",
    "viewer.add_image(data_image, name='CT Image', blending='additive')\n",
    "\n",
    "# Ajouter le masque (en ajustant la transparence)\n",
    "viewer.add_image(data_mask, name='DL Mask', blending='additive', opacity=0.5)\n",
    "\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e70402",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Grad-cam (basic version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf8e43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradcam(model, image, target_layer_name):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.input],\n",
    "        [model.get_layer(target_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(np.expand_dims(image, axis=0))\n",
    "        # Use mean prediction over the output map for loss\n",
    "        loss = tf.reduce_mean(predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "    conv_outputs = conv_outputs[0]\n",
    "\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    cam = np.zeros(conv_outputs.shape[:2], dtype=np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * conv_outputs[:, :, i]\n",
    "\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (np.max(cam) + 1e-8)\n",
    "    cam = cv2.resize(cam, (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    return cam\n",
    "\n",
    "def show_gradcam_overlay(image, cam, alpha=0.4):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    # Convert grayscale to BGR (3 channels)\n",
    "    if len(image.shape) == 2:\n",
    "        image_color = cv2.cvtColor(np.uint8(image * 255), cv2.COLOR_GRAY2BGR)\n",
    "    else:\n",
    "        image_color = np.uint8(image * 255)\n",
    "    overlay = cv2.addWeighted(image_color, 1-alpha, heatmap, alpha, 0)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Grad-CAM Overlay\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae0ffa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Middle slice shape: {middle_slice.shape}\")\n",
    "# --- Prepare the slice for the model ---\n",
    "input_slice = middle_slice.reshape(512, 512, 1).astype(np.float32)\n",
    "\n",
    "# --- Compute Grad-CAM ---\n",
    "cam = compute_gradcam(model.model1, input_slice, target_layer_name=\"conv2d_23\")\n",
    "\n",
    "# --- Visualize overlay ---\n",
    "show_gradcam_overlay(input_slice.squeeze(), cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cbb8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xplique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94395b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xplique.attributions import (Saliency, GradientInput, IntegratedGradients, SmoothGrad, VarGrad, SquareGrad,\n",
    "                                  Occlusion, Rise, SobolAttributionMethod, HsicAttributionMethod)\n",
    "\n",
    "from xplique.plots import plot_attributions\n",
    "\n",
    "img_tf = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "img_tf = tf.expand_dims(img_tf, axis=-1)  # Now shape is [38, 512, 512, 1]\n",
    "mask_tf = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "mask_alpha = 0.5\n",
    "images_with_masks = (1 - mask_alpha) * img_tf + mask_alpha * mask_tf\n",
    "\n",
    "explainers = {\n",
    "    Saliency: {},\n",
    "    GradientInput: {},\n",
    "    IntegratedGradients: {\"steps\": 20},\n",
    "    SmoothGrad: {\"nb_samples\": 50, \"noise\": 0.75},\n",
    "    VarGrad: {\"nb_samples\": 50, \"noise\": 0.75},\n",
    "    SquareGrad: {\"nb_samples\": 100, \"noise\": 0.5},\n",
    "    Occlusion: {\"patch_size\": 40, \"patch_stride\": 10, \"occlusion_value\": 0}, -> to long to run\n",
    "    Rise: {\"nb_samples\": 4000, \"grid_size\": 13}, -> to long to run\n",
    "    SobolAttributionMethod: {\"nb_design\": 32, \"grid_size\": 13},\n",
    "    HsicAttributionMethod: {\"nb_design\": 1500, \"grid_size\": 13}\n",
    "}\n",
    "\n",
    "explanations = {}\n",
    "for explainer_class, params in explainers.items():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(explainer_class.__name__)\n",
    "\n",
    "    # instanciate explainer\n",
    "    explainer = explainer_class(model.model1, operator=xplique.Tasks.SEMANTIC_SEGMENTATION,\n",
    "                                batch_size=1, **params)\n",
    "\n",
    "    # compute explanations\n",
    "    explanation = explainer(img, mask)\n",
    "\n",
    "    # show explanations for a method\n",
    "    plot_attributions(explanation, images_with_masks,\n",
    "                      img_size=4., cols=img_tf.shape[0],\n",
    "                      cmap='jet', alpha=0.3, absolute_value=False, clip_percentile=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # keep explanations in memory for metrics\n",
    "    explanations[explainer_class.__name__] = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbddeb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xplique.metrics import Deletion, MuFidelity, Insertion, AverageStability\n",
    "from xplique.plots.metrics import barplot\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# -------------------- MuFidelity's nb_samples was reduced for memory needs\n",
    "# explanations metrics\n",
    "explanations_metrics = {\n",
    "    Deletion: {\"baseline_mode\": 0, \"steps\": 10, \"max_percentage_perturbed\": 0.5},\n",
    "    MuFidelity: {\"baseline_mode\": 0, \"nb_samples\": 5, \"subset_percent\":0.2, \"grid_size\": 13},\n",
    "    Insertion: {\"baseline_mode\": 0, \"steps\": 10, \"max_percentage_perturbed\": 0.5}\n",
    "}\n",
    "for metric_class, params in explanations_metrics.items():\n",
    "    torch.cuda.empty_cache()\n",
    "    # instanciate the metric\n",
    "    metric = metric_class(model.model1, np.array(img_tf[:3]), np.array(mask_tf[:3]),\n",
    "                          operator=xplique.Tasks.SEMANTIC_SEGMENTATION,\n",
    "                          activation=\"softmax\", batch_size=BATCH_SIZE, **params)\n",
    "\n",
    "    # iterate on methods explanations\n",
    "    metrics[metric_class.__name__] = {}\n",
    "    for method, explanation in explanations.items():\n",
    "        metrics[metric_class.__name__][method] = metric(explanation[:3])\n",
    "\n",
    "# # ----------------- Not included for computation cost reason\n",
    "# explainer metrics\n",
    "    metric = AverageStability(model.model1, img_tf[:3], mask_tf[:3], batch_size=BATCH_SIZE,\n",
    "                           nb_samples=20, radius=0.1, distance=\"l2\")\n",
    "    metrics[\"AverageStability\"] = {}\n",
    "    for explainer_class, params in explainers.items():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "     # instanciate explainer\n",
    "    explainer = explainer_class(model.model1, operator=xplique.Tasks.SEMANTIC_SEGMENTATION,\n",
    "                                 batch_size=BATCH_SIZE, **params)\n",
    "\n",
    "    metrics[\"AverageStability\"][explainer_class.__name__] = metric.evaluate(explainer)\n",
    "\n",
    "barplot(metrics, sort_metric=\"Deletion\", ascending=\"True\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.414258,
   "end_time": "2025-05-31T20:30:13.217555",
   "environment_variables": {},
   "exception": null,
   "input_path": "/usr/users/pred_lung_cancer/piquet_con/Project-lung-cancer/src/Xai/Grad_Cam.ipynb",
   "output_path": "/usr/users/pred_lung_cancer/piquet_con/Project-lung-cancer/src/Xai/Grad_Cam_output.ipynb",
   "parameters": {},
   "start_time": "2025-05-31T20:30:12.803297",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}